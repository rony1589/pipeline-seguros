# ğŸ¦ Pipeline de Seguros - Data Engineer Challenge

Este proyecto implementa un pipeline **end-to-end** para procesar datos diarios de pÃ³lizas y siniestros en una aseguradora.

## ğŸš€ Objetivo

Procesar archivos CSV diarios (`polizas_YYYYMMDD.csv` y `siniestros_YYYYMMDD.csv`), validar su calidad, transformar los datos y cargarlos en **BigQuery**.

---

## ğŸ§± Estructura
```
pipeline-seguros/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ dags/
â”‚ â””â”€â”€ pipeline_polizas_dag.py
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ generar_datos.py
â”‚ â”œâ”€â”€ validacion.py
â”‚ â””â”€â”€ transformaciones.py
â”‚ â””â”€â”€ utils_auditoria.py
â”œâ”€â”€ sql/
â”‚ â””â”€â”€ create_tables.sql
â””â”€â”€ tests/
â”œâ”€â”€ test_generador.py
â”œâ”€â”€ test_validacion.py
â””â”€â”€ test_transformaciones.py
```
---

## Requisitos

- Python 3.10/3.11 (3.12 puede funcionar, pero algunos paquetes muestran problemas en Windows)
- Java JDK 11 (para PySpark)
- git, pip
- (Opcional) Airflow 2.6.3 para desplegar DAG

## InstalaciÃ³n local (recomendado usar Git Bash o PowerShell)

```bash
git clone https://github.com/rony1589/pipeline-seguros.git

cd pipeline-seguros

python -m venv .venv

# En Git Bash:
source .venv/Scripts/activate

# En PowerShell:
# .venv\Scripts\Activate.ps1

pip install -r requirements.txt


## ğŸ§ª EjecuciÃ³n Local

1ï¸âƒ£ Generar datos:
python -m scripts.generar_datos
# genera files en data/ por defecto

2ï¸âƒ£ Validar archivos generados:
# reemplaza YYYYMMDD si quieres, o usa los nombres generados
python -m scripts.validacion --pol data/polizas_YYYYMMDD.csv --sin data/siniestros_YYYYMMDD.csv

Ejemplo: python -m scripts.validacion --pol data/polizas_20251027.csv --sin data/siniestros_20251027.csv

3ï¸âƒ£ Transformar y auditar:
python -m scripts.transformaciones --pol data/polizas_YYYYMMDD.csv --sin data/siniestros_YYYYMMDD.csv --out data

Ejemplo: python -m scripts.transformaciones --pol data/polizas_20251027.csv --sin data/siniestros_20251027.csv --out data


4ï¸âƒ£  Ejecutar pruebas unitarias
python -m pytest -q
o
python -m pytest -v


## ğŸ§© Flujo del Pipeline

1. **Generar Datos SintÃ©ticos** â†’ `generar_datos.py`
2. **Validar Estructura y Calidad** â†’ `validacion.py`
3. **Transformar con PySpark/Pandas** â†’ `transformaciones.py`
4. **Ejecutar DAG Airflow** â†’ `pipeline_polizas_dag.py`
5. **Cargar en BigQuery** â†’ `create_tables.sql`

---

## ğŸ§  Decisiones TÃ©cnicas

- **Particionamiento por fecha** en BigQuery â†’ optimiza queries diarias.
- **Validaciones automÃ¡ticas** â†’ asegura calidad de datos (>10% errores = cuarentena).
- **Branching en Airflow** â†’ si datos corruptos â†’ mover_cuarentena / si vÃ¡lidos â†’ procesar.
- **PySpark local** â†’ procesar y resumir informaciÃ³n sin depender de un cluster.

---

ğŸ§  ExplicaciÃ³n del Funcionamiento

generar_datos.py: crea CSVs sintÃ©ticos con 5% errores simulados.

validacion.py: detecta errores y marca dataset como vÃ¡lido o corrupto.

transformaciones.py: combina pÃ³lizas y siniestros, genera resumen por producto.

pipeline_polizas_dag.py: orquesta las tareas diarias en Airflow.

create_tables.sql: define estructura final en BigQuery.

tests/: validan consistencia de generaciÃ³n, validaciÃ³n y transformaciÃ³n.

```
